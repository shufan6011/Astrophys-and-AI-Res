{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNR6I5q5nV1VyKEqsGuco+o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shufan6011/ML-Projects/blob/main/Step_4_CNNs_for_GW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update:\n",
        "# Use GPU instead of CPU for model training"
      ],
      "metadata": {
        "id": "DRV2KeFQyHFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "FzL41q-39psp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95dylknOM1YX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests, os\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import butter, filtfilt, spectrogram\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Go to https://gwosc.org\n",
        "# Find the information required below (GPS time & detector)\n"
      ],
      "metadata": {
        "id": "bzbKy-scjhEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOWxRKp9QYKj"
      },
      "outputs": [],
      "source": [
        "# Set a GPS time:\n",
        "t_start = 1126259462.4\n",
        "t_end = 1126259462.4 # For specific events, make t_end the same as t_start\n",
        "\n",
        "# Choose detector as H1, L1, or V1\n",
        "detector = 'H1'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q31hDw3QYKn"
      },
      "outputs": [],
      "source": [
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "try:\n",
        "    from gwpy.timeseries import TimeSeries\n",
        "except:\n",
        "    ! pip install -q \"gwpy==3.0.8\"\n",
        "    ! pip install -q \"matplotlib==3.9.0\"\n",
        "    ! pip install -q \"astropy==6.1.0\"\n",
        "    from gwpy.timeseries import TimeSeries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhembQ-1QYKo"
      },
      "outputs": [],
      "source": [
        "from gwosc.locate import get_urls\n",
        "url = get_urls(detector, t_start, t_end)[-1]\n",
        "\n",
        "print('Downloading: ' , url)\n",
        "fn = os.path.basename(url)\n",
        "with open(fn,'wb') as strainfile:\n",
        "    straindata = requests.get(url)\n",
        "    strainfile.write(straindata.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WB3QLymQYKq"
      },
      "outputs": [],
      "source": [
        "# Read strain data\n",
        "strain = TimeSeries.read(fn,format='hdf5.gwosc')\n",
        "\n",
        "# Examine an interval of the event closely\n",
        "# center = int(t_start)\n",
        "# strain = strain.crop(center-0.2, center+0.1)\n",
        "\n",
        "# Extract timestamps and strain values\n",
        "timestamps = strain.times.value\n",
        "strain_values = strain.value\n",
        "\n",
        "# Store the data in a Pandas DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'time': timestamps,\n",
        "    'strain': strain_values\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Missing Values"
      ],
      "metadata": {
        "id": "st3QjZwnPSs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing values\n",
        "data = data.dropna()\n",
        "\n",
        "print(\"\\nMissing values after cleaning:\")\n",
        "print(data.isnull().sum())\n"
      ],
      "metadata": {
        "id": "qgcP-cYXNKzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Noise Filtering"
      ],
      "metadata": {
        "id": "o9GWFMK9PBtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Band-pass filter function\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    try:\n",
        "        nyq = 0.5 * fs\n",
        "        low = lowcut / nyq\n",
        "        high = highcut / nyq\n",
        "        b, a = butter(order, [low, high], btype='band')\n",
        "        return b, a\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "def bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    try:\n",
        "        b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "        y = filtfilt(b, a, data)\n",
        "        return y\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "# Filter parameters\n",
        "lowcut = 20  # Low cutoff frequency (Hz)\n",
        "highcut = 500  # High cutoff frequency (Hz)\n",
        "\n",
        "# Apply band-pass filter to the strain data\n",
        "data['strain'] = bandpass_filter(data['strain'], lowcut, highcut, 4096)\n"
      ],
      "metadata": {
        "id": "zQOeTtFfNkDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Normalization"
      ],
      "metadata": {
        "id": "VZjAUoRqft9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the filtered strain data\n",
        "scaler = StandardScaler()\n",
        "data['strain'] = scaler.fit_transform(data[['strain']])\n"
      ],
      "metadata": {
        "id": "OTmnUun1fwK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Inspection"
      ],
      "metadata": {
        "id": "qrpSAW3aPNzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect the first few rows\n",
        "print(\"First few rows of the data:\")\n",
        "print(data.head())\n",
        "\n",
        "# Inspect col headers\n",
        "print(\"\\nCol headers:\")\n",
        "print(data.columns)\n",
        "\n",
        "# Summary stats\n",
        "print(\"\\nSummary stats:\")\n",
        "print(data.describe())\n",
        "\n",
        "# Check for missing vals\n",
        "print(\"\\nMissing vals in each col:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Check the sampling frequency\n",
        "print(f\"\\nSampling frequency: {strain.sample_rate} Hz\")\n",
        "fs = 4096 # Change this if sampling frequency is different\n"
      ],
      "metadata": {
        "id": "QTyLuzDQJxfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1D CNN"
      ],
      "metadata": {
        "id": "JrEtLCfYiW-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segment Labeling"
      ],
      "metadata": {
        "id": "1gFfsPAVZdVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the preprocessed data back to original strain\n",
        "scaler.inverse_transform(data)\n",
        "\n",
        "# Window Size\n",
        "window_size = 2  # in seconds, adjust as needed\n",
        "\n",
        "# Define event time and window half-length\n",
        "event_time = t_start  # Adjust as needed\\\n",
        "\n",
        "# Create segments and labels\n",
        "segments = []\n",
        "labels = []\n",
        "\n",
        "for i in range(0, len(strain) - int(window_size * fs), int(window_size * fs)):\n",
        "    segment = strain[i:i + int(window_size * fs)]\n",
        "    segments.append(segment.value)\n",
        "\n",
        "    # Label based on event presence\n",
        "    if (segment.times.value[0] <= event_time <= segment.times.value[-1]):\n",
        "        labels.append(1)  # Event present\n",
        "    else:\n",
        "        labels.append(0)  # No event\n",
        "\n",
        "# Convert to numpy arrays\n",
        "segments = np.array(segments)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(f\"Segments shape: {segments.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "# Verify segments and labels\n",
        "print(\"First few segments:\")\n",
        "print(segments[:2])\n"
      ],
      "metadata": {
        "id": "ltYtG_vPojJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation (time-series data)"
      ],
      "metadata": {
        "id": "UtFOkh04eBvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape segments for 1D CNN\n",
        "segments = segments.reshape((segments.shape[0], segments.shape[1], 1))\n",
        "\n",
        "print(f\"Reshaped segments shape: {segments.shape}\")\n"
      ],
      "metadata": {
        "id": "Rp-ijCrde0kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(segments, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "MqK55ujwKe5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "NaOSgzTgkly3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_data(data, labels):\n",
        "    try:\n",
        "        augmented_data = []\n",
        "        augmented_labels = []\n",
        "        for d, l in zip(data, labels):\n",
        "            augmented_data.append(d)\n",
        "            augmented_labels.append(l)\n",
        "            augmented_data.append(np.flip(d, axis=0))\n",
        "            augmented_labels.append(l)\n",
        "            noise = np.random.normal(0, 0.1, d.shape)\n",
        "            augmented_data.append(d + noise)\n",
        "            augmented_labels.append(l)\n",
        "        return np.array(augmented_data), np.array(augmented_labels)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "X_train_aug, y_train_aug = augment_data(X_train, y_train)\n",
        "\n",
        "print(f\"Original training data shape: {X_train.shape}\")\n",
        "print(f\"Augmented training data shape: {X_train_aug.shape}\")\n"
      ],
      "metadata": {
        "id": "3us6ddy7koxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training & Evaluation"
      ],
      "metadata": {
        "id": "SUiygYr9wXcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the 1D CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(16, 3, activation='relu', input_shape=(X_train_aug.shape[1], 1)),\n",
        "    MaxPooling1D(2),\n",
        "    Conv1D(32, 3, activation='relu'),\n",
        "    MaxPooling1D(2),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_aug, y_train_aug, epochs=20, batch_size=256, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xb-akVQXwW_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Product"
      ],
      "metadata": {
        "id": "laI0KcJ6kskp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model (look in the left panel)\n",
        "model.save('gw_1d_cnn.h5')\n",
        "\n",
        "# Load the model (for verification)\n",
        "loaded_model = tf.keras.models.load_model('gw_1d_cnn.h5')\n",
        "\n",
        "# Verify the loaded model by evaluating it on the test set\n",
        "loaded_loss, loaded_accuracy = loaded_model.evaluate(X_test, y_test)\n",
        "print(f\"Loaded Model Test Loss: {loaded_loss}, Loaded Model Test Accuracy: {loaded_accuracy}\")\n"
      ],
      "metadata": {
        "id": "BiCFbj8zkwLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2D CNN"
      ],
      "metadata": {
        "id": "aL18nvsThsBo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Segment Labeling"
      ],
      "metadata": {
        "id": "kw14mwJqdVuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create segments and labels\n",
        "segments = []\n",
        "labels = []\n",
        "\n",
        "for i in range(0, len(strain) - int(window_size * fs), int(window_size * fs)):\n",
        "    segment = strain[i:i + int(window_size * fs)]\n",
        "    segments.append(segment.value)\n",
        "\n",
        "    # Label based on event presence\n",
        "    if (segment.times.value[0] <= event_time <= segment.times.value[-1]):\n",
        "        labels.append(1)  # Event present\n",
        "    else:\n",
        "        labels.append(0)  # No event\n",
        "\n",
        "# Convert to numpy arrays\n",
        "segments = np.array(segments)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(f\"Segments shape: {segments.shape}\")\n",
        "print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "# Verify segments and labels\n",
        "print(\"First few segments:\")\n",
        "print(segments[:2])\n"
      ],
      "metadata": {
        "id": "hYH0dHEVdVuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation (spectrograms)"
      ],
      "metadata": {
        "id": "LNqRVKdYilIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate spectrograms for each segment\n",
        "def generate_spectrogram(segment, sample_rate):\n",
        "    try:\n",
        "        f, t, Sxx = spectrogram(segment, sample_rate)\n",
        "        return Sxx\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "spectrograms = np.array([generate_spectrogram(segment, fs) for segment in segments])\n",
        "print(f\"Spectrograms shape: {spectrograms.shape}\")\n",
        "\n",
        "# Reshape spectrograms for 2D CNN\n",
        "spectrograms = spectrograms[..., np.newaxis]  # Add a channel dimension\n",
        "print(f\"Reshaped spectrograms shape: {spectrograms.shape}\")"
      ],
      "metadata": {
        "id": "6yuzkBn-GI43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(spectrograms, labels, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "1aF3O-DaKblq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "rcG9dOpVh0rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_data(data, labels):\n",
        "    try:\n",
        "        augmented_data = []\n",
        "        augmented_labels = []\n",
        "        for d, l in zip(data, labels):\n",
        "            augmented_data.append(d)\n",
        "            augmented_labels.append(l)\n",
        "            augmented_data.append(np.flip(d, axis=0))\n",
        "            augmented_labels.append(l)\n",
        "            noise = np.random.normal(0, 0.1, d.shape)\n",
        "            augmented_data.append(d + noise)\n",
        "            augmented_labels.append(l)\n",
        "        return np.array(augmented_data), np.array(augmented_labels)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "X_train_aug, y_train_aug = augment_data(X_train, y_train)\n",
        "\n",
        "print(f\"Original training data shape: {X_train.shape}\")\n",
        "print(f\"Augmented training data shape: {X_train_aug.shape}\")\n"
      ],
      "metadata": {
        "id": "oCv8EJEph0rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training & Evaluation"
      ],
      "metadata": {
        "id": "Arp5fvieh0rE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the 2D CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(16, (3, 3), activation='relu', input_shape=(X_train_aug.shape[1], spectrograms.shape[2], 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_aug, y_train_aug, epochs=20, batch_size=256, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WYrX1L38h0rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Product"
      ],
      "metadata": {
        "id": "kCELXDbBh0rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model (look in the left panel)\n",
        "model.save('gw_2d_cnn.h5')\n",
        "\n",
        "# Load the model (for verification)\n",
        "loaded_model = tf.keras.models.load_model('gw_2d_cnn.h5')\n",
        "\n",
        "# Verify the loaded model by evaluating it on the test set\n",
        "loaded_loss, loaded_accuracy = loaded_model.evaluate(X_test, y_test)\n",
        "print(f\"Loaded Model Test Loss: {loaded_loss}, Loaded Model Test Accuracy: {loaded_accuracy}\")\n"
      ],
      "metadata": {
        "id": "ozOL87Feh0rF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}